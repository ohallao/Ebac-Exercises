{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b3362ed2-9104-427d-b7cb-2a924491d8b2",
      "metadata": {
        "id": "b3362ed2-9104-427d-b7cb-2a924491d8b2"
      },
      "source": [
        "## 1. Cite 5 diferenças entre o o AdaBoost e o GBM\n",
        "\n",
        "*   Modelo Base:\n",
        "\n",
        "AdaBoost (AB): Os modelos são treinados em cima de \"stumps\" (árvores de decisão com apenas um nível).\n",
        "\n",
        "Gradient Boosting Machine (GBM): Utiliza árvores de decisão completas, embora geralmente sejam rasas para evitar overfitting.\n",
        "\n",
        "*   Variáveis Explicativas:\n",
        "\n",
        "AB: Os stumps são criados para cada variável explicativa de forma independente.\n",
        "\n",
        "GBM: As variáveis explicativas permanecem as mesmas em cada iteração, enquanto a variável alvo é ajustada através dos resíduos.\n",
        "\n",
        "*   Início do Processo:\n",
        "\n",
        "AB: Começa com um \"weak learner\", que é um modelo simples com desempenho ligeiramente melhor que o acaso.\n",
        "\n",
        "GBM: Começa com uma árvore de decisão inicial e melhora a performance ajustando os resíduos.\n",
        "\n",
        "*   Método de Atualização:\n",
        "\n",
        "AB: Utiliza um esquema de reamostragem onde cada observação recebe um peso, que é atualizado com base no desempenho do modelo anterior.\n",
        "\n",
        "GBM: Ajusta a variável alvo em cada iteração somando os resíduos multiplicados por uma taxa de aprendizado, que controla o impacto de cada árvore na previsão final.\n",
        "\n",
        "*   Influência entre Modelos:\n",
        "\n",
        "AB: Cada modelo treinado influencia no próximo, pois os pesos das observações são ajustados.\n",
        "\n",
        "GBM: Cada modelo é treinado para corrigir os erros do modelo anterior ajustando os resíduos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d349da-db7e-474b-9e7b-ec561144d262",
      "metadata": {
        "id": "58d349da-db7e-474b-9e7b-ec561144d262"
      },
      "source": [
        "## 2. Acesse o link [Scikit-learn – GBM](https://scikit-learn.org/stable/modules/ensemble.html), leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3abad99-4f07-43c7-885b-e45134da6440",
      "metadata": {
        "tags": [],
        "id": "f3abad99-4f07-43c7-885b-e45134da6440",
        "outputId": "d15a2723-4e95-4831-92ca-078af914e264"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.913"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "X, y = make_hastie_10_2(random_state=0)\n",
        "X_train, X_test = X[:2000], X[2000:]\n",
        "y_train, y_test = y[:2000], y[2000:]\n",
        "\n",
        "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a2a370-131c-4416-9477-0268018490ad",
      "metadata": {
        "id": "67a2a370-131c-4416-9477-0268018490ad"
      },
      "source": [
        "## 3. Cite 5 Hyperparametros importantes no AdaBoost.\n",
        "\n",
        "- n_estimators (Quantidade de modelos)\n",
        "- learning_rate (Taxa de aprendizado)\n",
        "- max_features (Quantidade máxima de variáveis explicativas)\n",
        "- max_depth (Profundidade da árvore dos modelos)\n",
        "- min_samples_leaf (Mínimo de amostras por folha)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c466ab8-e9a8-47a4-b757-255868896c70",
      "metadata": {
        "id": "8c466ab8-e9a8-47a4-b757-255868896c70"
      },
      "source": [
        "## 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a38c1e-4da3-4ee9-a76d-ed2ac062d127",
      "metadata": {
        "id": "94a38c1e-4da3-4ee9-a76d-ed2ac062d127",
        "outputId": "829d99fe-424d-4f81-f09f-525a0878a7d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 5.81 s\n",
            "Wall time: 3min 18s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'learning_rate': 0.1,\n",
              " 'max_depth': 8,\n",
              " 'max_features': 1,\n",
              " 'min_samples_leaf': 50,\n",
              " 'n_estimators': 100}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\": [10, 50, 100],\n",
        "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
        "    \"max_depth\": [3, 5, 8],\n",
        "    \"min_samples_leaf\": [5, 10, 20, 50],\n",
        "    \"max_features\": [1, 5]\n",
        "}\n",
        "\n",
        "clf = GradientBoostingClassifier()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = clf,\n",
        "    param_grid=params,\n",
        "    n_jobs=-1,\n",
        "    cv=2,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid_result = grid_search.fit(X, y)\n",
        "grid_result.best_score_\n",
        "grid_result.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "256288e8-a8aa-4d3a-ab79-7de76468836d",
      "metadata": {
        "id": "256288e8-a8aa-4d3a-ab79-7de76468836d"
      },
      "source": [
        "# 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0383acf-e437-4d01-878a-cd6b80f29dcc",
      "metadata": {
        "tags": [],
        "id": "f0383acf-e437-4d01-878a-cd6b80f29dcc"
      },
      "source": [
        "Enquanto o GMB utiliza a mesma base de dados para todos os modelos,  o Stochastic GBM utiliza um subconjuntos com uma amostra aleatória sem repetição"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}